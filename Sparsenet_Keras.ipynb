{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sparsenet_Keras.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShoaibSheriff/Computer-Vision/blob/master/Sparsenet_Keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "reUuj_s8IsER",
        "colab_type": "code",
        "outputId": "2ecf9a91-cc33-432c-effd-37b2b9fa19f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "'''SparseNet models for Keras.\n",
        "# Reference\n",
        "- [Sparsely Connected Convolutional Networks](https://arxiv.org/abs/1801.05895)\n",
        "- [Github](https://github.com/lyken17/sparsenet)\n",
        "'''\n",
        "from __future__ import print_function\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "\n",
        "import numpy as np\n",
        "import warnings\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.layers.core import Dense, Dropout, Activation, Reshape\n",
        "from keras.layers.convolutional import Conv2D, Conv2DTranspose, UpSampling2D\n",
        "from keras.layers.pooling import AveragePooling2D, MaxPooling2D\n",
        "from keras.layers.pooling import GlobalAveragePooling2D\n",
        "from keras.layers import Input\n",
        "from keras.layers.merge import concatenate\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.regularizers import l2\n",
        "from keras.utils.layer_utils import convert_all_kernels_in_model, convert_dense_weights_data_format\n",
        "from keras.utils.data_utils import get_file\n",
        "from keras.engine.topology import get_source_inputs\n",
        "from keras.applications.imagenet_utils import _obtain_input_shape\n",
        "from keras.applications.imagenet_utils import decode_predictions\n",
        "import keras.backend as K\n",
        "\n",
        "\n",
        "def preprocess_input(x, data_format=None):\n",
        "    \"\"\"Preprocesses a tensor encoding a batch of images.\n",
        "    # Arguments\n",
        "        x: input Numpy tensor, 4D.\n",
        "        data_format: data format of the image tensor.\n",
        "    # Returns\n",
        "        Preprocessed tensor.\n",
        "    \"\"\"\n",
        "    if data_format is None:\n",
        "        data_format = K.image_data_format()\n",
        "    assert data_format in {'channels_last', 'channels_first'}\n",
        "\n",
        "    if data_format == 'channels_first':\n",
        "        if x.ndim == 3:\n",
        "            # 'RGB'->'BGR'\n",
        "            x = x[::-1, ...]\n",
        "            # Zero-center by mean pixel\n",
        "            x[0, :, :] -= 103.939\n",
        "            x[1, :, :] -= 116.779\n",
        "            x[2, :, :] -= 123.68\n",
        "        else:\n",
        "            x = x[:, ::-1, ...]\n",
        "            x[:, 0, :, :] -= 103.939\n",
        "            x[:, 1, :, :] -= 116.779\n",
        "            x[:, 2, :, :] -= 123.68\n",
        "    else:\n",
        "        # 'RGB'->'BGR'\n",
        "        x = x[..., ::-1]\n",
        "        # Zero-center by mean pixel\n",
        "        x[..., 0] -= 103.939\n",
        "        x[..., 1] -= 116.779\n",
        "        x[..., 2] -= 123.68\n",
        "\n",
        "    x *= 0.017  # scale values\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def SparseNet(input_shape=None, depth=40, nb_dense_block=3, growth_rate=12, nb_filter=-1, nb_layers_per_block=-1,\n",
        "              bottleneck=False, reduction=0.0, dropout_rate=0.0, weight_decay=1e-4, subsample_initial_block=False,\n",
        "              include_top=True, weights=None, input_tensor=None,\n",
        "              classes=10, activation='softmax'):\n",
        "    '''Instantiate the SparseNet architecture,\n",
        "        optionally loading weights pre-trained\n",
        "        on CIFAR-10. Note that when using TensorFlow,\n",
        "        for best performance you should set\n",
        "        `image_data_format='channels_last'` in your Keras config\n",
        "        at ~/.keras/keras.json.\n",
        "        The model and the weights are compatible with both\n",
        "        TensorFlow and Theano. The dimension ordering\n",
        "        convention used by the model is the one\n",
        "        specified in your Keras config file.\n",
        "        # Arguments\n",
        "            input_shape: optional shape tuple, only to be specified\n",
        "                if `include_top` is False (otherwise the input shape\n",
        "                has to be `(32, 32, 3)` (with `channels_last` dim ordering)\n",
        "                or `(3, 32, 32)` (with `channels_first` dim ordering).\n",
        "                It should have exactly 3 inputs channels,\n",
        "                and width and height should be no smaller than 8.\n",
        "                E.g. `(200, 200, 3)` would be one valid value.\n",
        "            depth: number or layers in the DenseNet\n",
        "            nb_dense_block: number of dense blocks to add to end (generally = 3)\n",
        "            growth_rate: number of filters to add per dense block. Can be\n",
        "                a single integer number or a list of numbers.\n",
        "                If it is a list, length of list must match the length of\n",
        "                `nb_layers_per_block`\n",
        "            nb_filter: initial number of filters. -1 indicates initial\n",
        "                number of filters is 2 * growth_rate\n",
        "            nb_layers_per_block: number of layers in each dense block.\n",
        "                Can be a -1, positive integer or a list.\n",
        "                If -1, calculates nb_layer_per_block from the network depth.\n",
        "                If positive integer, a set number of layers per dense block.\n",
        "                If list, nb_layer is used as provided. Note that list size must\n",
        "                be (nb_dense_block + 1)\n",
        "            bottleneck: flag to add bottleneck blocks in between dense blocks\n",
        "            reduction: reduction factor of transition blocks.\n",
        "                Note : reduction value is inverted to compute compression.\n",
        "            dropout_rate: dropout rate\n",
        "            weight_decay: weight decay rate\n",
        "            subsample_initial_block: Set to True to subsample the initial convolution and\n",
        "                add a MaxPool2D before the dense blocks are added.\n",
        "            include_top: whether to include the fully-connected\n",
        "                layer at the top of the network.\n",
        "            weights: one of `None` (random initialization) or\n",
        "                'imagenet' (pre-training on ImageNet)..\n",
        "            input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)\n",
        "                to use as image input for the model.\n",
        "            classes: optional number of classes to classify images\n",
        "                into, only to be specified if `include_top` is True, and\n",
        "                if no `weights` argument is specified.\n",
        "            activation: Type of activation at the top layer. Can be one of 'softmax' or 'sigmoid'.\n",
        "                Note that if sigmoid is used, classes must be 1.\n",
        "        # Returns\n",
        "            A Keras model instance.\n",
        "        '''\n",
        "\n",
        "    if weights not in {'imagenet', None}:\n",
        "        raise ValueError('The `weights` argument should be either '\n",
        "                         '`None` (random initialization) or `cifar10` '\n",
        "                         '(pre-training on CIFAR-10).')\n",
        "\n",
        "    if weights == 'imagenet' and include_top and classes != 1000:\n",
        "        raise ValueError('If using `weights` as ImageNet with `include_top`'\n",
        "                         ' as true, `classes` should be 1000')\n",
        "\n",
        "    if activation not in ['softmax', 'sigmoid']:\n",
        "        raise ValueError('activation must be one of \"softmax\" or \"sigmoid\"')\n",
        "\n",
        "    if activation == 'sigmoid' and classes != 1:\n",
        "        raise ValueError('sigmoid activation can only be used when classes = 1')\n",
        "\n",
        "    # Determine proper input shape\n",
        "    input_shape = _obtain_input_shape(input_shape,\n",
        "                                      default_size=32,\n",
        "                                      min_size=8,\n",
        "                                      data_format=K.image_data_format(),\n",
        "                                      require_flatten=include_top)\n",
        "\n",
        "    if input_tensor is None:\n",
        "        img_input = Input(shape=input_shape)\n",
        "    else:\n",
        "        if not K.is_keras_tensor(input_tensor):\n",
        "            img_input = Input(tensor=input_tensor, shape=input_shape)\n",
        "        else:\n",
        "            img_input = input_tensor\n",
        "\n",
        "    x = _create_dense_net(classes, img_input, include_top, depth, nb_dense_block,\n",
        "                          growth_rate, nb_filter, nb_layers_per_block, bottleneck, reduction,\n",
        "                          dropout_rate, weight_decay, subsample_initial_block, activation)\n",
        "\n",
        "    # Ensure that the model takes into account\n",
        "    # any potential predecessors of `input_tensor`.\n",
        "    if input_tensor is not None:\n",
        "        inputs = get_source_inputs(input_tensor)\n",
        "    else:\n",
        "        inputs = img_input\n",
        "    # Create model.\n",
        "    model = Model(inputs, x, name='densenet')\n",
        "\n",
        "    # load weights\n",
        "    if weights == 'imagenet':\n",
        "        weights_loaded = False\n",
        "\n",
        "        if weights_loaded:\n",
        "            if K.backend() == 'theano':\n",
        "                convert_all_kernels_in_model(model)\n",
        "\n",
        "            if K.image_data_format() == 'channels_first' and K.backend() == 'tensorflow':\n",
        "                warnings.warn('You are using the TensorFlow backend, yet you '\n",
        "                              'are using the Theano '\n",
        "                              'image data format convention '\n",
        "                              '(`image_data_format=\"channels_first\"`). '\n",
        "                              'For best performance, set '\n",
        "                              '`image_data_format=\"channels_last\"` in '\n",
        "                              'your Keras config '\n",
        "                              'at ~/.keras/keras.json.')\n",
        "\n",
        "            print(\"Weights for the model were loaded successfully\")\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def _exponential_index_fetch(x_list):\n",
        "    count = len(x_list)\n",
        "    i = 1\n",
        "    inputs = []\n",
        "    while i <= count:\n",
        "        inputs.append(x_list[count - i])\n",
        "        i *= 2\n",
        "    return inputs\n",
        "\n",
        "\n",
        "def _conv_block(ip, nb_filter, bottleneck=False, dropout_rate=None, weight_decay=1e-4):\n",
        "    ''' Apply BatchNorm, Relu, 3x3 Conv2D, optional bottleneck block and dropout\n",
        "    Args:\n",
        "        ip: Input keras tensor\n",
        "        nb_filter: number of filters\n",
        "        bottleneck: add bottleneck block\n",
        "        dropout_rate: dropout rate\n",
        "        weight_decay: weight decay factor\n",
        "    Returns: keras tensor with batch_norm, relu and convolution2d added (optional bottleneck)\n",
        "    '''\n",
        "    concat_axis = 1 if K.image_data_format() == 'channels_first' else -1\n",
        "\n",
        "    with K.name_scope('conv_block'):\n",
        "        x = BatchNormalization(axis=concat_axis, momentum=0.1, epsilon=1e-5)(ip)\n",
        "        x = Activation('relu')(x)\n",
        "\n",
        "        if bottleneck:\n",
        "            inter_channel = nb_filter * 4  # Obtained from https://github.com/liuzhuang13/DenseNet/blob/master/densenet.lua\n",
        "\n",
        "            x = Conv2D(inter_channel, (1, 1), kernel_initializer='he_normal', padding='same', use_bias=False,\n",
        "                       kernel_regularizer=l2(weight_decay))(x)\n",
        "            x = BatchNormalization(axis=concat_axis, epsilon=1e-5, momentum=0.1)(x)\n",
        "            x = Activation('relu')(x)\n",
        "\n",
        "        x = Conv2D(nb_filter, (3, 3), kernel_initializer='he_normal', padding='same', use_bias=False)(x)\n",
        "        if dropout_rate:\n",
        "            x = Dropout(dropout_rate)(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def _dense_block(x, nb_layers, nb_filter, growth_rate, bottleneck=False, dropout_rate=None, weight_decay=1e-4,\n",
        "                 grow_nb_filters=True, return_concat_list=False):\n",
        "    ''' Build a dense_block where the output of each conv_block is fed to subsequent ones\n",
        "    Args:\n",
        "        x: keras tensor\n",
        "        nb_layers: the number of layers of conv_block to append to the model.\n",
        "        nb_filter: number of filters\n",
        "        growth_rate: growth rate\n",
        "        bottleneck: bottleneck block\n",
        "        dropout_rate: dropout rate\n",
        "        weight_decay: weight decay factor\n",
        "        grow_nb_filters: flag to decide to allow number of filters to grow\n",
        "        return_concat_list: return the list of feature maps along with the actual output\n",
        "    Returns: keras tensor with nb_layers of conv_block appended\n",
        "    '''\n",
        "    concat_axis = 1 if K.image_data_format() == 'channels_first' else -1\n",
        "\n",
        "    x_list = [x]\n",
        "    channel_list = [nb_filter]\n",
        "\n",
        "    for i in range(nb_layers):\n",
        "        #nb_channels = sum(_exponential_index_fetch(channel_list))\n",
        "\n",
        "        x = _conv_block(x, growth_rate, bottleneck, dropout_rate, weight_decay)\n",
        "        x_list.append(x)\n",
        "\n",
        "        fetch_outputs = _exponential_index_fetch(x_list)\n",
        "        x = concatenate(fetch_outputs, axis=concat_axis)\n",
        "\n",
        "        channel_list.append(growth_rate)\n",
        "\n",
        "    if grow_nb_filters:\n",
        "        nb_filter = sum(_exponential_index_fetch(channel_list))\n",
        "\n",
        "    if return_concat_list:\n",
        "        return x, nb_filter, x_list\n",
        "    else:\n",
        "        return x, nb_filter\n",
        "\n",
        "\n",
        "def _transition_block(ip, nb_filter, compression=1.0, weight_decay=1e-4):\n",
        "    ''' Apply BatchNorm, Relu 1x1, Conv2D, optional compression, dropout and Maxpooling2D\n",
        "    Args:\n",
        "        ip: keras tensor\n",
        "        nb_filter: number of filters\n",
        "        compression: calculated as 1 - reduction. Reduces the number of feature maps\n",
        "                    in the transition block.\n",
        "        dropout_rate: dropout rate\n",
        "        weight_decay: weight decay factor\n",
        "    Returns: keras tensor, after applying batch_norm, relu-conv, dropout, maxpool\n",
        "    '''\n",
        "    concat_axis = 1 if K.image_data_format() == 'channels_first' else -1\n",
        "\n",
        "    with K.name_scope('transition_block'):\n",
        "        x = BatchNormalization(axis=concat_axis, epsilon=1e-5, momentum=0.1)(ip)\n",
        "        x = Activation('relu')(x)\n",
        "        x = Conv2D(int(nb_filter * compression), (1, 1), kernel_initializer='he_normal', padding='same', use_bias=False,\n",
        "                   kernel_regularizer=l2(weight_decay))(x)\n",
        "        x = AveragePooling2D((2, 2), strides=(2, 2))(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def _create_dense_net(nb_classes, img_input, include_top, depth=40, nb_dense_block=3, growth_rate=12, nb_filter=-1,\n",
        "                      nb_layers_per_block=-1, bottleneck=False, reduction=0.0, dropout_rate=None, weight_decay=1e-4,\n",
        "                      subsample_initial_block=False, activation='softmax'):\n",
        "    ''' Build the DenseNet model\n",
        "    Args:\n",
        "        nb_classes: number of classes\n",
        "        img_input: tuple of shape (channels, rows, columns) or (rows, columns, channels)\n",
        "        include_top: flag to include the final Dense layer\n",
        "        depth: number or layers\n",
        "        nb_dense_block: number of dense blocks to add to end (generally = 3)\n",
        "        growth_rate: number of filters to add per dense block\n",
        "        nb_filter: initial number of filters. Default -1 indicates initial number of filters is 2 * growth_rate\n",
        "        nb_layers_per_block: number of layers in each dense block.\n",
        "                Can be a -1, positive integer or a list.\n",
        "                If -1, calculates nb_layer_per_block from the depth of the network.\n",
        "                If positive integer, a set number of layers per dense block.\n",
        "                If list, nb_layer is used as provided. Note that list size must\n",
        "                be (nb_dense_block + 1)\n",
        "        bottleneck: add bottleneck blocks\n",
        "        reduction: reduction factor of transition blocks. Note : reduction value is inverted to compute compression\n",
        "        dropout_rate: dropout rate\n",
        "        weight_decay: weight decay rate\n",
        "        subsample_initial_block: Set to True to subsample the initial convolution and\n",
        "                add a MaxPool2D before the dense blocks are added.\n",
        "        subsample_initial:\n",
        "        activation: Type of activation at the top layer. Can be one of 'softmax' or 'sigmoid'.\n",
        "                Note that if sigmoid is used, classes must be 1.\n",
        "    Returns: keras tensor with nb_layers of conv_block appended\n",
        "    '''\n",
        "\n",
        "    concat_axis = 1 if K.image_data_format() == 'channels_first' else -1\n",
        "\n",
        "    if reduction != 0.0:\n",
        "        assert reduction <= 1.0 and reduction > 0.0, 'reduction value must lie between 0.0 and 1.0'\n",
        "\n",
        "    # layers in each dense block\n",
        "    if type(nb_layers_per_block) is list or type(nb_layers_per_block) is tuple:\n",
        "        nb_layers = list(nb_layers_per_block)  # Convert tuple to list\n",
        "\n",
        "        assert len(nb_layers) == (nb_dense_block), 'If list, nb_layer is used as provided. ' \\\n",
        "                                                   'Note that list size must be (nb_dense_block)'\n",
        "        final_nb_layer = nb_layers[-1]\n",
        "        nb_layers = nb_layers[:-1]\n",
        "    else:\n",
        "        if nb_layers_per_block == -1:\n",
        "            assert (depth - 4) % 3 == 0, 'Depth must be 3 N + 4 if nb_layers_per_block == -1'\n",
        "            count = int((depth - 4) / 3)\n",
        "\n",
        "            if bottleneck:\n",
        "                count = count // 2\n",
        "\n",
        "            nb_layers = [count for _ in range(nb_dense_block)]\n",
        "            final_nb_layer = count\n",
        "        else:\n",
        "            final_nb_layer = nb_layers_per_block\n",
        "            nb_layers = [nb_layers_per_block] * nb_dense_block\n",
        "\n",
        "    if type(growth_rate) is list or type(growth_rate) is tuple:\n",
        "        growth_rate = list(growth_rate)\n",
        "        assert len(growth_rate) == len(nb_layers)\n",
        "    else:\n",
        "        growth_rate = [growth_rate for _ in range(len(nb_layers))]\n",
        "\n",
        "    # compute initial nb_filter if -1, else accept users initial nb_filter\n",
        "    if nb_filter <= 0:\n",
        "        nb_filter = 2 * growth_rate[0]\n",
        "\n",
        "    # compute compression factor\n",
        "    compression = 1.0 - reduction\n",
        "\n",
        "    # Initial convolution\n",
        "    if subsample_initial_block:\n",
        "        initial_kernel = (7, 7)\n",
        "        initial_strides = (2, 2)\n",
        "    else:\n",
        "        initial_kernel = (3, 3)\n",
        "        initial_strides = (1, 1)\n",
        "\n",
        "    x = Conv2D(nb_filter, initial_kernel, kernel_initializer='he_normal', padding='same',\n",
        "               strides=initial_strides, use_bias=False, kernel_regularizer=l2(weight_decay))(img_input)\n",
        "\n",
        "    if subsample_initial_block:\n",
        "        x = BatchNormalization(axis=concat_axis, epsilon=1e-5, momentum=0.1)(x)\n",
        "        x = Activation('relu')(x)\n",
        "        x = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
        "\n",
        "    # Add dense blocks\n",
        "    for block_idx in range(nb_dense_block - 1):\n",
        "        x, nb_filter = _dense_block(x, nb_layers[block_idx], nb_filter, growth_rate[block_idx], bottleneck=bottleneck,\n",
        "                                    dropout_rate=dropout_rate, weight_decay=weight_decay)\n",
        "        # add transition_block\n",
        "        x = _transition_block(x, nb_filter, compression=compression, weight_decay=weight_decay)\n",
        "        nb_filter = int(nb_filter * compression)\n",
        "\n",
        "    # The last dense_block does not have a transition_block\n",
        "    x, nb_filter = _dense_block(x, final_nb_layer, nb_filter, growth_rate[-1], bottleneck=bottleneck,\n",
        "                                dropout_rate=dropout_rate, weight_decay=weight_decay)\n",
        "\n",
        "    x = BatchNormalization(axis=concat_axis, epsilon=1e-5, momentum=0.1)(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "\n",
        "    if include_top:\n",
        "        x = Dense(nb_classes, activation=activation)(x)\n",
        "\n",
        "    return x\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "kW_IqQ-GJFH3",
        "colab_type": "code",
        "outputId": "ba2aecfa-bf4c-4464-8b5a-31c774deff7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 7021
        }
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import os.path\n",
        "\n",
        "import numpy as np\n",
        "import sklearn.metrics as metrics\n",
        "\n",
        "from keras.datasets import cifar10\n",
        "from keras.utils import np_utils\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.optimizers import Adam, SGD\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "from keras import backend as K\n",
        "\n",
        "batch_size = 100\n",
        "nb_classes = 10\n",
        "nb_epoch = 100\n",
        "\n",
        "img_rows, img_cols = 32, 32\n",
        "img_channels = 3\n",
        "\n",
        "img_dim = (img_channels, img_rows, img_cols) if K.image_dim_ordering() == \"th\" else (img_rows, img_cols, img_channels)\n",
        "depth = 40\n",
        "nb_dense_block = 3\n",
        "growth_rate = 24\n",
        "nb_filter = -1\n",
        "dropout_rate = 0.0  # 0.0 for data augmentation\n",
        "\n",
        "model = SparseNet(img_dim, classes=nb_classes, depth=depth, nb_dense_block=nb_dense_block,\n",
        "                            growth_rate=growth_rate, nb_filter=nb_filter, dropout_rate=dropout_rate, weights=None)\n",
        "print(\"Model created\")\n",
        "\n",
        "model.summary()\n",
        "optimizer = Adam(lr=5e-3, amsgrad=True)  # Using Adam instead of SGD to speed up training\n",
        "#optimizer = SGD(lr=0.1, decay=0.0001, momentum=0.9, nesterov=True)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=[\"accuracy\"])\n",
        "print(\"Finished compiling\")\n",
        "print(\"Building model...\")\n",
        "\n",
        "(trainX, trainY), (testX, testY) = cifar10.load_data()\n",
        "\n",
        "trainX = trainX.astype('float32')\n",
        "testX = testX.astype('float32')\n",
        "\n",
        "trainX = preprocess_input(trainX)\n",
        "testX = preprocess_input(testX)\n",
        "\n",
        "cifar_mean = trainX.mean(axis=(0, 1, 2), keepdims=True)\n",
        "cifar_std = trainX.std(axis=(0, 1, 2), keepdims=True)\n",
        "\n",
        "trainX = (trainX - cifar_mean) / (cifar_std + 1e-8)\n",
        "testX = (testX - cifar_mean) / (cifar_std + 1e-8)\n",
        "\n",
        "Y_train = np_utils.to_categorical(trainY, nb_classes)\n",
        "Y_test = np_utils.to_categorical(testY, nb_classes)\n",
        "\n",
        "generator = ImageDataGenerator(width_shift_range=8. / 32,\n",
        "                               height_shift_range=8. / 32,\n",
        "                               horizontal_flip=True)\n",
        "\n",
        "generator.fit(trainX, seed=0)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model created\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 32, 32, 48)   1296        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 32, 32, 48)   192         conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 32, 32, 48)   0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 32, 32, 24)   10368       activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 32, 32, 72)   0           conv2d_2[0][0]                   \n",
            "                                                                 conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 32, 32, 72)   288         concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 32, 32, 72)   0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 32, 32, 24)   15552       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 32, 32, 48)   0           conv2d_3[0][0]                   \n",
            "                                                                 conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 32, 32, 48)   192         concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 32, 32, 48)   0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 32, 32, 24)   10368       activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 32, 32, 96)   0           conv2d_4[0][0]                   \n",
            "                                                                 conv2d_3[0][0]                   \n",
            "                                                                 conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 32, 32, 96)   384         concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 32, 32, 96)   0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 32, 32, 24)   20736       activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_4 (Concatenate)     (None, 32, 32, 72)   0           conv2d_5[0][0]                   \n",
            "                                                                 conv2d_4[0][0]                   \n",
            "                                                                 conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 32, 32, 72)   288         concatenate_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 32, 32, 72)   0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 32, 32, 24)   15552       activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_5 (Concatenate)     (None, 32, 32, 72)   0           conv2d_6[0][0]                   \n",
            "                                                                 conv2d_5[0][0]                   \n",
            "                                                                 conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 32, 32, 72)   288         concatenate_5[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 32, 32, 72)   0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 32, 32, 24)   15552       activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_6 (Concatenate)     (None, 32, 32, 72)   0           conv2d_7[0][0]                   \n",
            "                                                                 conv2d_6[0][0]                   \n",
            "                                                                 conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 32, 32, 72)   288         concatenate_6[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 32, 32, 72)   0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 32, 32, 24)   15552       activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_7 (Concatenate)     (None, 32, 32, 120)  0           conv2d_8[0][0]                   \n",
            "                                                                 conv2d_7[0][0]                   \n",
            "                                                                 conv2d_5[0][0]                   \n",
            "                                                                 conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 32, 32, 120)  480         concatenate_7[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 32, 32, 120)  0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 32, 32, 24)   25920       activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_8 (Concatenate)     (None, 32, 32, 96)   0           conv2d_9[0][0]                   \n",
            "                                                                 conv2d_8[0][0]                   \n",
            "                                                                 conv2d_6[0][0]                   \n",
            "                                                                 conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 32, 32, 96)   384         concatenate_8[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 32, 32, 96)   0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 32, 32, 24)   20736       activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_9 (Concatenate)     (None, 32, 32, 96)   0           conv2d_10[0][0]                  \n",
            "                                                                 conv2d_9[0][0]                   \n",
            "                                                                 conv2d_7[0][0]                   \n",
            "                                                                 conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 32, 32, 96)   384         concatenate_9[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 32, 32, 96)   0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 32, 32, 24)   20736       activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_10 (Concatenate)    (None, 32, 32, 96)   0           conv2d_11[0][0]                  \n",
            "                                                                 conv2d_10[0][0]                  \n",
            "                                                                 conv2d_8[0][0]                   \n",
            "                                                                 conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 32, 32, 96)   384         concatenate_10[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 32, 32, 96)   0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 32, 32, 24)   20736       activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_11 (Concatenate)    (None, 32, 32, 96)   0           conv2d_12[0][0]                  \n",
            "                                                                 conv2d_11[0][0]                  \n",
            "                                                                 conv2d_9[0][0]                   \n",
            "                                                                 conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 32, 32, 96)   384         concatenate_11[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 32, 32, 96)   0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 32, 32, 24)   20736       activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_12 (Concatenate)    (None, 32, 32, 96)   0           conv2d_13[0][0]                  \n",
            "                                                                 conv2d_12[0][0]                  \n",
            "                                                                 conv2d_10[0][0]                  \n",
            "                                                                 conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 32, 32, 96)   384         concatenate_12[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 32, 32, 96)   0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 32, 32, 96)   9216        activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_1 (AveragePoo (None, 16, 16, 96)   0           conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 16, 16, 96)   384         average_pooling2d_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 16, 16, 96)   0           batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 16, 16, 24)   20736       activation_14[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_13 (Concatenate)    (None, 16, 16, 120)  0           conv2d_15[0][0]                  \n",
            "                                                                 average_pooling2d_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 16, 16, 120)  480         concatenate_13[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 16, 16, 120)  0           batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 16, 16, 24)   25920       activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_14 (Concatenate)    (None, 16, 16, 48)   0           conv2d_16[0][0]                  \n",
            "                                                                 conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 16, 16, 48)   192         concatenate_14[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 16, 16, 48)   0           batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 16, 16, 24)   10368       activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_15 (Concatenate)    (None, 16, 16, 144)  0           conv2d_17[0][0]                  \n",
            "                                                                 conv2d_16[0][0]                  \n",
            "                                                                 average_pooling2d_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 16, 16, 144)  576         concatenate_15[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 16, 16, 144)  0           batch_normalization_17[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 16, 16, 24)   31104       activation_17[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_16 (Concatenate)    (None, 16, 16, 72)   0           conv2d_18[0][0]                  \n",
            "                                                                 conv2d_17[0][0]                  \n",
            "                                                                 conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_18 (BatchNo (None, 16, 16, 72)   288         concatenate_16[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 16, 16, 72)   0           batch_normalization_18[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 16, 16, 24)   15552       activation_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_17 (Concatenate)    (None, 16, 16, 72)   0           conv2d_19[0][0]                  \n",
            "                                                                 conv2d_18[0][0]                  \n",
            "                                                                 conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_19 (BatchNo (None, 16, 16, 72)   288         concatenate_17[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 16, 16, 72)   0           batch_normalization_19[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 16, 16, 24)   15552       activation_19[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_18 (Concatenate)    (None, 16, 16, 72)   0           conv2d_20[0][0]                  \n",
            "                                                                 conv2d_19[0][0]                  \n",
            "                                                                 conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_20 (BatchNo (None, 16, 16, 72)   288         concatenate_18[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_20 (Activation)      (None, 16, 16, 72)   0           batch_normalization_20[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 16, 16, 24)   15552       activation_20[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_19 (Concatenate)    (None, 16, 16, 168)  0           conv2d_21[0][0]                  \n",
            "                                                                 conv2d_20[0][0]                  \n",
            "                                                                 conv2d_18[0][0]                  \n",
            "                                                                 average_pooling2d_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_21 (BatchNo (None, 16, 16, 168)  672         concatenate_19[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_21 (Activation)      (None, 16, 16, 168)  0           batch_normalization_21[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 16, 16, 24)   36288       activation_21[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_20 (Concatenate)    (None, 16, 16, 96)   0           conv2d_22[0][0]                  \n",
            "                                                                 conv2d_21[0][0]                  \n",
            "                                                                 conv2d_19[0][0]                  \n",
            "                                                                 conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_22 (BatchNo (None, 16, 16, 96)   384         concatenate_20[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_22 (Activation)      (None, 16, 16, 96)   0           batch_normalization_22[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_23 (Conv2D)              (None, 16, 16, 24)   20736       activation_22[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_21 (Concatenate)    (None, 16, 16, 96)   0           conv2d_23[0][0]                  \n",
            "                                                                 conv2d_22[0][0]                  \n",
            "                                                                 conv2d_20[0][0]                  \n",
            "                                                                 conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_23 (BatchNo (None, 16, 16, 96)   384         concatenate_21[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_23 (Activation)      (None, 16, 16, 96)   0           batch_normalization_23[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_24 (Conv2D)              (None, 16, 16, 24)   20736       activation_23[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_22 (Concatenate)    (None, 16, 16, 96)   0           conv2d_24[0][0]                  \n",
            "                                                                 conv2d_23[0][0]                  \n",
            "                                                                 conv2d_21[0][0]                  \n",
            "                                                                 conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_24 (BatchNo (None, 16, 16, 96)   384         concatenate_22[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_24 (Activation)      (None, 16, 16, 96)   0           batch_normalization_24[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_25 (Conv2D)              (None, 16, 16, 24)   20736       activation_24[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_23 (Concatenate)    (None, 16, 16, 96)   0           conv2d_25[0][0]                  \n",
            "                                                                 conv2d_24[0][0]                  \n",
            "                                                                 conv2d_22[0][0]                  \n",
            "                                                                 conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_25 (BatchNo (None, 16, 16, 96)   384         concatenate_23[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_25 (Activation)      (None, 16, 16, 96)   0           batch_normalization_25[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_26 (Conv2D)              (None, 16, 16, 24)   20736       activation_25[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_24 (Concatenate)    (None, 16, 16, 96)   0           conv2d_26[0][0]                  \n",
            "                                                                 conv2d_25[0][0]                  \n",
            "                                                                 conv2d_23[0][0]                  \n",
            "                                                                 conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_26 (BatchNo (None, 16, 16, 96)   384         concatenate_24[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_26 (Activation)      (None, 16, 16, 96)   0           batch_normalization_26[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_27 (Conv2D)              (None, 16, 16, 96)   9216        activation_26[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_2 (AveragePoo (None, 8, 8, 96)     0           conv2d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_27 (BatchNo (None, 8, 8, 96)     384         average_pooling2d_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_27 (Activation)      (None, 8, 8, 96)     0           batch_normalization_27[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_28 (Conv2D)              (None, 8, 8, 24)     20736       activation_27[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_25 (Concatenate)    (None, 8, 8, 120)    0           conv2d_28[0][0]                  \n",
            "                                                                 average_pooling2d_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_28 (BatchNo (None, 8, 8, 120)    480         concatenate_25[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_28 (Activation)      (None, 8, 8, 120)    0           batch_normalization_28[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_29 (Conv2D)              (None, 8, 8, 24)     25920       activation_28[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_26 (Concatenate)    (None, 8, 8, 48)     0           conv2d_29[0][0]                  \n",
            "                                                                 conv2d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_29 (BatchNo (None, 8, 8, 48)     192         concatenate_26[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_29 (Activation)      (None, 8, 8, 48)     0           batch_normalization_29[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_30 (Conv2D)              (None, 8, 8, 24)     10368       activation_29[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_27 (Concatenate)    (None, 8, 8, 144)    0           conv2d_30[0][0]                  \n",
            "                                                                 conv2d_29[0][0]                  \n",
            "                                                                 average_pooling2d_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_30 (BatchNo (None, 8, 8, 144)    576         concatenate_27[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_30 (Activation)      (None, 8, 8, 144)    0           batch_normalization_30[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_31 (Conv2D)              (None, 8, 8, 24)     31104       activation_30[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_28 (Concatenate)    (None, 8, 8, 72)     0           conv2d_31[0][0]                  \n",
            "                                                                 conv2d_30[0][0]                  \n",
            "                                                                 conv2d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_31 (BatchNo (None, 8, 8, 72)     288         concatenate_28[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_31 (Activation)      (None, 8, 8, 72)     0           batch_normalization_31[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_32 (Conv2D)              (None, 8, 8, 24)     15552       activation_31[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_29 (Concatenate)    (None, 8, 8, 72)     0           conv2d_32[0][0]                  \n",
            "                                                                 conv2d_31[0][0]                  \n",
            "                                                                 conv2d_29[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_32 (BatchNo (None, 8, 8, 72)     288         concatenate_29[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_32 (Activation)      (None, 8, 8, 72)     0           batch_normalization_32[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_33 (Conv2D)              (None, 8, 8, 24)     15552       activation_32[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_30 (Concatenate)    (None, 8, 8, 72)     0           conv2d_33[0][0]                  \n",
            "                                                                 conv2d_32[0][0]                  \n",
            "                                                                 conv2d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_33 (BatchNo (None, 8, 8, 72)     288         concatenate_30[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_33 (Activation)      (None, 8, 8, 72)     0           batch_normalization_33[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_34 (Conv2D)              (None, 8, 8, 24)     15552       activation_33[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_31 (Concatenate)    (None, 8, 8, 168)    0           conv2d_34[0][0]                  \n",
            "                                                                 conv2d_33[0][0]                  \n",
            "                                                                 conv2d_31[0][0]                  \n",
            "                                                                 average_pooling2d_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_34 (BatchNo (None, 8, 8, 168)    672         concatenate_31[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_34 (Activation)      (None, 8, 8, 168)    0           batch_normalization_34[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_35 (Conv2D)              (None, 8, 8, 24)     36288       activation_34[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_32 (Concatenate)    (None, 8, 8, 96)     0           conv2d_35[0][0]                  \n",
            "                                                                 conv2d_34[0][0]                  \n",
            "                                                                 conv2d_32[0][0]                  \n",
            "                                                                 conv2d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_35 (BatchNo (None, 8, 8, 96)     384         concatenate_32[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_35 (Activation)      (None, 8, 8, 96)     0           batch_normalization_35[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_36 (Conv2D)              (None, 8, 8, 24)     20736       activation_35[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_33 (Concatenate)    (None, 8, 8, 96)     0           conv2d_36[0][0]                  \n",
            "                                                                 conv2d_35[0][0]                  \n",
            "                                                                 conv2d_33[0][0]                  \n",
            "                                                                 conv2d_29[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_36 (BatchNo (None, 8, 8, 96)     384         concatenate_33[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_36 (Activation)      (None, 8, 8, 96)     0           batch_normalization_36[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_37 (Conv2D)              (None, 8, 8, 24)     20736       activation_36[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_34 (Concatenate)    (None, 8, 8, 96)     0           conv2d_37[0][0]                  \n",
            "                                                                 conv2d_36[0][0]                  \n",
            "                                                                 conv2d_34[0][0]                  \n",
            "                                                                 conv2d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_37 (BatchNo (None, 8, 8, 96)     384         concatenate_34[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_37 (Activation)      (None, 8, 8, 96)     0           batch_normalization_37[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_38 (Conv2D)              (None, 8, 8, 24)     20736       activation_37[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_35 (Concatenate)    (None, 8, 8, 96)     0           conv2d_38[0][0]                  \n",
            "                                                                 conv2d_37[0][0]                  \n",
            "                                                                 conv2d_35[0][0]                  \n",
            "                                                                 conv2d_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_38 (BatchNo (None, 8, 8, 96)     384         concatenate_35[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_38 (Activation)      (None, 8, 8, 96)     0           batch_normalization_38[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_39 (Conv2D)              (None, 8, 8, 24)     20736       activation_38[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_36 (Concatenate)    (None, 8, 8, 96)     0           conv2d_39[0][0]                  \n",
            "                                                                 conv2d_38[0][0]                  \n",
            "                                                                 conv2d_36[0][0]                  \n",
            "                                                                 conv2d_32[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_39 (BatchNo (None, 8, 8, 96)     384         concatenate_36[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_39 (Activation)      (None, 8, 8, 96)     0           batch_normalization_39[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_1 (Glo (None, 96)           0           activation_39[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 10)           970         global_average_pooling2d_1[0][0] \n",
            "==================================================================================================\n",
            "Total params: 755,770\n",
            "Trainable params: 748,522\n",
            "Non-trainable params: 7,248\n",
            "__________________________________________________________________________________________________\n",
            "Finished compiling\n",
            "Building model...\n",
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "     8192/170498071 [..............................] - ETA: 20:01"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "170500096/170498071 [==============================] - 35s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-MYiquApPfHZ",
        "colab_type": "code",
        "outputId": "dbd65817-f7b6-4e8e-fff2-b3f502274526",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        }
      },
      "cell_type": "code",
      "source": [
        "model_file = \"SparseNet-40-24-CIFAR10.h5\"\n",
        "\n",
        "lr_reducer = ReduceLROnPlateau(monitor='val_acc', factor=0.5,\n",
        "                               cooldown=0, patience=4, min_lr=1e-9, verbose = 1)\n",
        "model_checkpoint = ModelCheckpoint(model_file, monitor=\"val_acc\", save_best_only=True, verbose=1)\n",
        "\n",
        "callbacks = [model_checkpoint, lr_reducer]\n",
        "\n",
        "model.fit_generator(generator.flow(trainX, Y_train, batch_size=batch_size),\n",
        "                    steps_per_epoch=len(trainX) // batch_size, epochs=30,\n",
        "                    callbacks=callbacks,\n",
        "                    validation_data=(testX, Y_test),\n",
        "                    validation_steps=testX.shape[0] // batch_size, verbose=1)\n",
        "\n",
        "from keras.models import load_model\n",
        "modelB = load_model(model_file)\n",
        "\n",
        "''''yPreds = modelB.predict(testX)\n",
        "yPred = np.argmax(yPreds, axis=1)\n",
        "yTrue = testY\n",
        "\n",
        "accuracy = metrics.accuracy_score(yTrue, yPred) * 100\n",
        "error = 100 - accuracy\n",
        "print(\"Accuracy : \", accuracy)\n",
        "print(\"Error : \", error)'''\n",
        "\n",
        "\n",
        "modelB.save(\"till30.h5\")\n",
        "from google.colab import files\n",
        "files.download(\"till30.h5\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "500/500 [==============================] - 236s 472ms/step - loss: 1.6998 - acc: 0.3799 - val_loss: 1.4386 - val_acc: 0.4874\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.48740, saving model to SparseNet-40-24-CIFAR10.h5\n",
            "Epoch 2/30\n",
            "165/500 [========>.....................] - ETA: 2:21 - loss: 1.3532 - acc: 0.5182"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 225s 451ms/step - loss: 1.2431 - acc: 0.5621 - val_loss: 1.3373 - val_acc: 0.5771\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.48740 to 0.57710, saving model to SparseNet-40-24-CIFAR10.h5\n",
            "Epoch 3/30\n",
            "229/500 [============>.................] - ETA: 1:54 - loss: 1.0562 - acc: 0.6348"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 225s 450ms/step - loss: 1.0156 - acc: 0.6494 - val_loss: 0.9813 - val_acc: 0.6731\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.57710 to 0.67310, saving model to SparseNet-40-24-CIFAR10.h5\n",
            "Epoch 4/30\n",
            "245/500 [=============>................] - ETA: 1:47 - loss: 0.8918 - acc: 0.6971"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 225s 450ms/step - loss: 0.8661 - acc: 0.7058 - val_loss: 0.9143 - val_acc: 0.6981\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.67310 to 0.69810, saving model to SparseNet-40-24-CIFAR10.h5\n",
            "Epoch 5/30\n",
            "249/500 [=============>................] - ETA: 1:45 - loss: 0.7748 - acc: 0.7378"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 225s 450ms/step - loss: 0.7698 - acc: 0.7407 - val_loss: 0.9296 - val_acc: 0.7126\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.69810 to 0.71260, saving model to SparseNet-40-24-CIFAR10.h5\n",
            "Epoch 6/30\n",
            "250/500 [==============>...............] - ETA: 1:45 - loss: 0.7067 - acc: 0.7645"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 224s 449ms/step - loss: 0.6878 - acc: 0.7720 - val_loss: 0.7012 - val_acc: 0.7699\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.71260 to 0.76990, saving model to SparseNet-40-24-CIFAR10.h5\n",
            "Epoch 7/30\n",
            "251/500 [==============>...............] - ETA: 1:44 - loss: 0.6439 - acc: 0.7871"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 224s 449ms/step - loss: 0.6361 - acc: 0.7900 - val_loss: 0.6803 - val_acc: 0.7817\n",
            "\n",
            "Epoch 00007: val_acc improved from 0.76990 to 0.78170, saving model to SparseNet-40-24-CIFAR10.h5\n",
            "Epoch 8/30\n",
            "251/500 [==============>...............] - ETA: 1:44 - loss: 0.5888 - acc: 0.8069"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 225s 450ms/step - loss: 0.5873 - acc: 0.8084 - val_loss: 0.6059 - val_acc: 0.8087\n",
            "\n",
            "Epoch 00008: val_acc improved from 0.78170 to 0.80870, saving model to SparseNet-40-24-CIFAR10.h5\n",
            "Epoch 9/30\n",
            "251/500 [==============>...............] - ETA: 1:45 - loss: 0.5510 - acc: 0.8229"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 225s 451ms/step - loss: 0.5507 - acc: 0.8229 - val_loss: 0.6663 - val_acc: 0.7995\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.80870\n",
            "Epoch 10/30\n",
            "296/500 [================>.............] - ETA: 1:26 - loss: 0.5295 - acc: 0.8289"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 225s 450ms/step - loss: 0.5220 - acc: 0.8315 - val_loss: 0.5627 - val_acc: 0.8277\n",
            "\n",
            "Epoch 00010: val_acc improved from 0.80870 to 0.82770, saving model to SparseNet-40-24-CIFAR10.h5\n",
            "Epoch 11/30\n",
            "256/500 [==============>...............] - ETA: 1:42 - loss: 0.5007 - acc: 0.8402"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dZhG_WcNMnS6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "modelB.save(\"till30.h5\")\n",
        "from google.colab import files\n",
        "files.download(\"till30.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "K1N1Ra_w2bvZ",
        "colab_type": "code",
        "outputId": "addf8d8a-51a4-4193-cbf7-a7b2f54df6b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2679
        }
      },
      "cell_type": "code",
      "source": [
        "model.fit_generator(generator.flow(trainX, Y_train, batch_size=batch_size),\n",
        "                    steps_per_epoch=len(trainX) // batch_size, epochs=30,\n",
        "                    callbacks=callbacks,\n",
        "                    validation_data=(testX, Y_test),\n",
        "                    validation_steps=testX.shape[0] // batch_size, verbose=1)\n",
        "\n",
        "from keras.models import load_model\n",
        "modelB = load_model(model_file)\n",
        "\n",
        "''''yPreds = modelB.predict(testX)\n",
        "yPred = np.argmax(yPreds, axis=1)\n",
        "yTrue = testY\n",
        "\n",
        "accuracy = metrics.accuracy_score(yTrue, yPred) * 100\n",
        "error = 100 - accuracy\n",
        "print(\"Accuracy : \", accuracy)\n",
        "print(\"Error : \", error)'''\n",
        "\n",
        "\n",
        "modelB.save(\"30to60.h5\")\n",
        "from google.colab import files\n",
        "files.download(\"30to60.h5\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "500/500 [==============================] - 217s 434ms/step - loss: 0.2701 - acc: 0.9165 - val_loss: 0.4179 - val_acc: 0.8804\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.88780\n",
            "Epoch 2/30\n",
            "193/500 [==========>...................] - ETA: 2:04 - loss: 0.2476 - acc: 0.9222"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 217s 433ms/step - loss: 0.2591 - acc: 0.9192 - val_loss: 0.4055 - val_acc: 0.8891\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.88780 to 0.88910, saving model to SparseNet-40-24-CIFAR10.h5\n",
            "Epoch 3/30\n",
            "236/500 [=============>................] - ETA: 1:47 - loss: 0.2504 - acc: 0.9217"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 217s 434ms/step - loss: 0.2554 - acc: 0.9204 - val_loss: 0.3711 - val_acc: 0.8932\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.88910 to 0.89320, saving model to SparseNet-40-24-CIFAR10.h5\n",
            "Epoch 4/30\n",
            "247/500 [=============>................] - ETA: 1:43 - loss: 0.2499 - acc: 0.9222"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 217s 435ms/step - loss: 0.2566 - acc: 0.9202 - val_loss: 0.3680 - val_acc: 0.8973\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.89320 to 0.89730, saving model to SparseNet-40-24-CIFAR10.h5\n",
            "Epoch 5/30\n",
            "250/500 [==============>...............] - ETA: 1:40 - loss: 0.2438 - acc: 0.9251"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 216s 433ms/step - loss: 0.2486 - acc: 0.9228 - val_loss: 0.4353 - val_acc: 0.8807\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.89730\n",
            "Epoch 6/30\n",
            "297/500 [================>.............] - ETA: 1:22 - loss: 0.2475 - acc: 0.9230"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 218s 436ms/step - loss: 0.2511 - acc: 0.9218 - val_loss: 0.4261 - val_acc: 0.8828\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.89730\n",
            "Epoch 7/30\n",
            "311/500 [=================>............] - ETA: 1:16 - loss: 0.2403 - acc: 0.9259"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 218s 435ms/step - loss: 0.2420 - acc: 0.9252 - val_loss: 0.3699 - val_acc: 0.8955\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.89730\n",
            "Epoch 8/30\n",
            "315/500 [=================>............] - ETA: 1:15 - loss: 0.2285 - acc: 0.9307"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 218s 436ms/step - loss: 0.2310 - acc: 0.9290 - val_loss: 0.3547 - val_acc: 0.9053\n",
            "\n",
            "Epoch 00008: val_acc improved from 0.89730 to 0.90530, saving model to SparseNet-40-24-CIFAR10.h5\n",
            "Epoch 9/30\n",
            "267/500 [===============>..............] - ETA: 1:37 - loss: 0.2335 - acc: 0.9307"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 224s 448ms/step - loss: 0.2367 - acc: 0.9290 - val_loss: 0.3687 - val_acc: 0.8987\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.90530\n",
            "Epoch 10/30\n",
            "301/500 [=================>............] - ETA: 1:22 - loss: 0.2280 - acc: 0.9301"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 220s 441ms/step - loss: 0.2266 - acc: 0.9307 - val_loss: 0.3683 - val_acc: 0.9015\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.90530\n",
            "Epoch 11/30\n",
            "311/500 [=================>............] - ETA: 1:18 - loss: 0.2148 - acc: 0.9343"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 221s 442ms/step - loss: 0.2245 - acc: 0.9312 - val_loss: 0.3985 - val_acc: 0.8929\n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.90530\n",
            "Epoch 12/30\n",
            "314/500 [=================>............] - ETA: 1:17 - loss: 0.2201 - acc: 0.9333"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 223s 445ms/step - loss: 0.2236 - acc: 0.9316 - val_loss: 0.3741 - val_acc: 0.8958\n",
            "\n",
            "Epoch 00012: val_acc did not improve from 0.90530\n",
            "Epoch 13/30\n",
            "315/500 [=================>............] - ETA: 1:18 - loss: 0.2195 - acc: 0.9341"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 224s 448ms/step - loss: 0.2224 - acc: 0.9331 - val_loss: 0.4029 - val_acc: 0.8937\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.90530\n",
            "\n",
            "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0010000000474974513.\n",
            "Epoch 14/30\n",
            "241/500 [=============>................] - ETA: 1:47 - loss: 0.1742 - acc: 0.9502"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 222s 443ms/step - loss: 0.1696 - acc: 0.9514 - val_loss: 0.3352 - val_acc: 0.9111\n",
            "\n",
            "Epoch 00014: val_acc improved from 0.90530 to 0.91110, saving model to SparseNet-40-24-CIFAR10.h5\n",
            "Epoch 15/30\n",
            "247/500 [=============>................] - ETA: 1:44 - loss: 0.1598 - acc: 0.9546"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 222s 443ms/step - loss: 0.1614 - acc: 0.9537 - val_loss: 0.3322 - val_acc: 0.9135\n",
            "\n",
            "Epoch 00015: val_acc improved from 0.91110 to 0.91350, saving model to SparseNet-40-24-CIFAR10.h5\n",
            "Epoch 16/30\n",
            "249/500 [=============>................] - ETA: 1:45 - loss: 0.1525 - acc: 0.9563"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 224s 449ms/step - loss: 0.1552 - acc: 0.9550 - val_loss: 0.3507 - val_acc: 0.9084\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.91350\n",
            "Epoch 17/30\n",
            "295/500 [================>.............] - ETA: 1:28 - loss: 0.1510 - acc: 0.9571"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 228s 455ms/step - loss: 0.1523 - acc: 0.9564 - val_loss: 0.3779 - val_acc: 0.9036\n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.91350\n",
            "Epoch 18/30\n",
            "309/500 [=================>............] - ETA: 1:18 - loss: 0.1532 - acc: 0.9553"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 221s 442ms/step - loss: 0.1528 - acc: 0.9551 - val_loss: 0.3710 - val_acc: 0.9069\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.91350\n",
            "Epoch 19/30\n",
            "313/500 [=================>............] - ETA: 1:18 - loss: 0.1502 - acc: 0.9563"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 223s 447ms/step - loss: 0.1507 - acc: 0.9566 - val_loss: 0.3934 - val_acc: 0.9049\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.91350\n",
            "Epoch 20/30\n",
            "314/500 [=================>............] - ETA: 1:16 - loss: 0.1429 - acc: 0.9582"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 221s 442ms/step - loss: 0.1415 - acc: 0.9587 - val_loss: 0.3594 - val_acc: 0.9082\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.91350\n",
            "\n",
            "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "Epoch 21/30\n",
            "242/500 [=============>................] - ETA: 1:45 - loss: 0.1269 - acc: 0.9647"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 221s 442ms/step - loss: 0.1258 - acc: 0.9648 - val_loss: 0.3401 - val_acc: 0.9174\n",
            "\n",
            "Epoch 00021: val_acc improved from 0.91350 to 0.91740, saving model to SparseNet-40-24-CIFAR10.h5\n",
            "Epoch 22/30\n",
            "248/500 [=============>................] - ETA: 1:45 - loss: 0.1161 - acc: 0.9688"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 226s 453ms/step - loss: 0.1184 - acc: 0.9677 - val_loss: 0.3236 - val_acc: 0.9172\n",
            "\n",
            "Epoch 00022: val_acc did not improve from 0.91740\n",
            "Epoch 23/30\n",
            "295/500 [================>.............] - ETA: 1:24 - loss: 0.1162 - acc: 0.9683"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 221s 441ms/step - loss: 0.1155 - acc: 0.9683 - val_loss: 0.3552 - val_acc: 0.9131\n",
            "\n",
            "Epoch 00023: val_acc did not improve from 0.91740\n",
            "Epoch 24/30\n",
            "309/500 [=================>............] - ETA: 1:19 - loss: 0.1118 - acc: 0.9705"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 223s 445ms/step - loss: 0.1102 - acc: 0.9707 - val_loss: 0.3471 - val_acc: 0.9156\n",
            "\n",
            "Epoch 00024: val_acc did not improve from 0.91740\n",
            "Epoch 25/30\n",
            "313/500 [=================>............] - ETA: 1:18 - loss: 0.1104 - acc: 0.9696"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 224s 448ms/step - loss: 0.1112 - acc: 0.9691 - val_loss: 0.3281 - val_acc: 0.9227\n",
            "\n",
            "Epoch 00025: val_acc improved from 0.91740 to 0.92270, saving model to SparseNet-40-24-CIFAR10.h5\n",
            "Epoch 26/30\n",
            "266/500 [==============>...............] - ETA: 1:36 - loss: 0.1110 - acc: 0.9705"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 221s 442ms/step - loss: 0.1120 - acc: 0.9698 - val_loss: 0.3316 - val_acc: 0.9191\n",
            "\n",
            "Epoch 00026: val_acc did not improve from 0.92270\n",
            "Epoch 27/30\n",
            "300/500 [=================>............] - ETA: 1:22 - loss: 0.1082 - acc: 0.9705"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 222s 443ms/step - loss: 0.1084 - acc: 0.9701 - val_loss: 0.3622 - val_acc: 0.9134\n",
            "\n",
            "Epoch 00027: val_acc did not improve from 0.92270\n",
            "Epoch 28/30\n",
            "310/500 [=================>............] - ETA: 1:19 - loss: 0.1057 - acc: 0.9704"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 223s 445ms/step - loss: 0.1061 - acc: 0.9706 - val_loss: 0.3916 - val_acc: 0.9092\n",
            "\n",
            "Epoch 00028: val_acc did not improve from 0.92270\n",
            "Epoch 29/30\n",
            "313/500 [=================>............] - ETA: 1:18 - loss: 0.1029 - acc: 0.9715"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 223s 446ms/step - loss: 0.1048 - acc: 0.9708 - val_loss: 0.3538 - val_acc: 0.9139\n",
            "\n",
            "Epoch 00029: val_acc did not improve from 0.92270\n",
            "Epoch 30/30\n",
            "314/500 [=================>............] - ETA: 1:16 - loss: 0.1037 - acc: 0.9717"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 223s 445ms/step - loss: 0.1039 - acc: 0.9715 - val_loss: 0.3635 - val_acc: 0.9157\n",
            "\n",
            "Epoch 00030: val_acc did not improve from 0.92270\n",
            "\n",
            "Epoch 00030: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-9a8343e71134>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mmodelB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"30to60.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"30to60.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0;34m'port'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m       \u001b[0;34m'path'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m       \u001b[0;34m'name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m   })\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result)\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     84\u001b[0m         reply.get('colab_msg_id') == message_id):\n\u001b[1;32m     85\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: TypeError: Failed to fetch"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "pWbOwXnPmXoG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "modelB.save(\"30to60.h5\")\n",
        "from google.colab import files\n",
        "files.download(\"30to60.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EA2qM3WANekw",
        "colab_type": "code",
        "outputId": "a69d9d82-02ab-4023-c648-20af45faa152",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2747
        }
      },
      "cell_type": "code",
      "source": [
        "model.fit_generator(generator.flow(trainX, Y_train, batch_size=batch_size),\n",
        "                    steps_per_epoch=len(trainX) // batch_size, epochs=30,\n",
        "                    callbacks=callbacks,\n",
        "                    validation_data=(testX, Y_test),\n",
        "                    validation_steps=testX.shape[0] // batch_size, verbose=1)\n",
        "\n",
        "from keras.models import load_model\n",
        "modelB = load_model(model_file)\n",
        "\n",
        "''''yPreds = modelB.predict(testX)\n",
        "yPred = np.argmax(yPreds, axis=1)\n",
        "yTrue = testY\n",
        "\n",
        "accuracy = metrics.accuracy_score(yTrue, yPred) * 100\n",
        "error = 100 - accuracy\n",
        "print(\"Accuracy : \", accuracy)\n",
        "print(\"Error : \", error)'''\n",
        "\n",
        "\n",
        "modelB.save(\"60to90.h5\")\n",
        "from google.colab import files\n",
        "files.download(\"60to90.h5\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "500/500 [==============================] - 222s 443ms/step - loss: 0.0960 - acc: 0.9746 - val_loss: 0.3557 - val_acc: 0.9159\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.92270\n",
            "Epoch 2/30\n",
            "193/500 [==========>...................] - ETA: 2:07 - loss: 0.0902 - acc: 0.9766"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 222s 443ms/step - loss: 0.0914 - acc: 0.9761 - val_loss: 0.3483 - val_acc: 0.9188\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.92270\n",
            "Epoch 3/30\n",
            "279/500 [===============>..............] - ETA: 1:32 - loss: 0.0889 - acc: 0.9781"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 223s 446ms/step - loss: 0.0909 - acc: 0.9771 - val_loss: 0.3630 - val_acc: 0.9143\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.92270\n",
            "Epoch 4/30\n",
            "305/500 [=================>............] - ETA: 1:22 - loss: 0.0883 - acc: 0.9768"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 224s 448ms/step - loss: 0.0883 - acc: 0.9772 - val_loss: 0.3687 - val_acc: 0.9136\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.92270\n",
            "Epoch 5/30\n",
            "313/500 [=================>............] - ETA: 1:17 - loss: 0.0891 - acc: 0.9765"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 224s 447ms/step - loss: 0.0893 - acc: 0.9762 - val_loss: 0.3796 - val_acc: 0.9163\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.92270\n",
            "Epoch 6/30\n",
            "316/500 [=================>............] - ETA: 1:17 - loss: 0.0882 - acc: 0.9771"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 222s 444ms/step - loss: 0.0878 - acc: 0.9770 - val_loss: 0.3624 - val_acc: 0.9168\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.92270\n",
            "Epoch 7/30\n",
            "316/500 [=================>............] - ETA: 1:14 - loss: 0.0869 - acc: 0.9764"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 218s 437ms/step - loss: 0.0886 - acc: 0.9761 - val_loss: 0.3672 - val_acc: 0.9181\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.92270\n",
            "\n",
            "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "Epoch 8/30\n",
            "243/500 [=============>................] - ETA: 1:44 - loss: 0.0840 - acc: 0.9785"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 218s 435ms/step - loss: 0.0833 - acc: 0.9783 - val_loss: 0.3461 - val_acc: 0.9217\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.92270\n",
            "Epoch 9/30\n",
            "294/500 [================>.............] - ETA: 1:23 - loss: 0.0824 - acc: 0.9790"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 218s 435ms/step - loss: 0.0833 - acc: 0.9784 - val_loss: 0.3566 - val_acc: 0.9197\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.92270\n",
            "Epoch 10/30\n",
            "309/500 [=================>............] - ETA: 1:18 - loss: 0.0790 - acc: 0.9805"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 222s 443ms/step - loss: 0.0789 - acc: 0.9804 - val_loss: 0.3395 - val_acc: 0.9249\n",
            "\n",
            "Epoch 00010: val_acc improved from 0.92270 to 0.92490, saving model to SparseNet-40-24-CIFAR10.h5\n",
            "Epoch 11/30\n",
            "265/500 [==============>...............] - ETA: 1:38 - loss: 0.0819 - acc: 0.9790"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 223s 446ms/step - loss: 0.0807 - acc: 0.9792 - val_loss: 0.3494 - val_acc: 0.9197\n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.92490\n",
            "Epoch 12/30\n",
            "300/500 [=================>............] - ETA: 1:22 - loss: 0.0759 - acc: 0.9816"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 221s 442ms/step - loss: 0.0772 - acc: 0.9812 - val_loss: 0.3275 - val_acc: 0.9233\n",
            "\n",
            "Epoch 00012: val_acc did not improve from 0.92490\n",
            "Epoch 13/30\n",
            "310/500 [=================>............] - ETA: 1:19 - loss: 0.0820 - acc: 0.9794"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 223s 446ms/step - loss: 0.0796 - acc: 0.9798 - val_loss: 0.3495 - val_acc: 0.9221\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.92490\n",
            "Epoch 14/30\n",
            "313/500 [=================>............] - ETA: 1:17 - loss: 0.0733 - acc: 0.9818"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 225s 450ms/step - loss: 0.0755 - acc: 0.9812 - val_loss: 0.3401 - val_acc: 0.9238\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.92490\n",
            "Epoch 15/30\n",
            "314/500 [=================>............] - ETA: 1:17 - loss: 0.0773 - acc: 0.9809"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 221s 442ms/step - loss: 0.0766 - acc: 0.9810 - val_loss: 0.3768 - val_acc: 0.9167\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.92490\n",
            "\n",
            "Epoch 00015: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "Epoch 16/30\n",
            "242/500 [=============>................] - ETA: 1:47 - loss: 0.0768 - acc: 0.9806"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 223s 446ms/step - loss: 0.0767 - acc: 0.9808 - val_loss: 0.3582 - val_acc: 0.9216\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.92490\n",
            "Epoch 17/30\n",
            "293/500 [================>.............] - ETA: 1:26 - loss: 0.0744 - acc: 0.9823"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 222s 444ms/step - loss: 0.0750 - acc: 0.9818 - val_loss: 0.3555 - val_acc: 0.9205\n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.92490\n",
            "Epoch 18/30\n",
            "308/500 [=================>............] - ETA: 1:19 - loss: 0.0724 - acc: 0.9821"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 221s 443ms/step - loss: 0.0730 - acc: 0.9818 - val_loss: 0.3545 - val_acc: 0.9210\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.92490\n",
            "Epoch 19/30\n",
            "313/500 [=================>............] - ETA: 1:17 - loss: 0.0741 - acc: 0.9811"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 223s 447ms/step - loss: 0.0732 - acc: 0.9816 - val_loss: 0.3617 - val_acc: 0.9202\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.92490\n",
            "Epoch 20/30\n",
            "314/500 [=================>............] - ETA: 1:18 - loss: 0.0751 - acc: 0.9815"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 224s 448ms/step - loss: 0.0756 - acc: 0.9812 - val_loss: 0.3662 - val_acc: 0.9210\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.92490\n",
            "\n",
            "Epoch 00020: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "Epoch 21/30\n",
            "242/500 [=============>................] - ETA: 1:47 - loss: 0.0730 - acc: 0.9812"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 222s 444ms/step - loss: 0.0747 - acc: 0.9806 - val_loss: 0.3476 - val_acc: 0.9216\n",
            "\n",
            "Epoch 00021: val_acc did not improve from 0.92490\n",
            "Epoch 22/30\n",
            "293/500 [================>.............] - ETA: 1:25 - loss: 0.0716 - acc: 0.9823"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 222s 445ms/step - loss: 0.0738 - acc: 0.9814 - val_loss: 0.3520 - val_acc: 0.9220\n",
            "\n",
            "Epoch 00022: val_acc did not improve from 0.92490\n",
            "Epoch 23/30\n",
            "308/500 [=================>............] - ETA: 1:21 - loss: 0.0725 - acc: 0.9820"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 225s 449ms/step - loss: 0.0729 - acc: 0.9819 - val_loss: 0.3601 - val_acc: 0.9191\n",
            "\n",
            "Epoch 00023: val_acc did not improve from 0.92490\n",
            "Epoch 24/30\n",
            "313/500 [=================>............] - ETA: 1:17 - loss: 0.0704 - acc: 0.9835"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 223s 446ms/step - loss: 0.0710 - acc: 0.9832 - val_loss: 0.3514 - val_acc: 0.9208\n",
            "\n",
            "Epoch 00024: val_acc did not improve from 0.92490\n",
            "Epoch 25/30\n",
            "314/500 [=================>............] - ETA: 1:17 - loss: 0.0732 - acc: 0.9815"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 222s 444ms/step - loss: 0.0745 - acc: 0.9811 - val_loss: 0.3644 - val_acc: 0.9190\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.92490\n",
            "\n",
            "Epoch 00025: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "Epoch 26/30\n",
            "241/500 [=============>................] - ETA: 1:49 - loss: 0.0749 - acc: 0.9810"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 224s 448ms/step - loss: 0.0742 - acc: 0.9814 - val_loss: 0.3529 - val_acc: 0.9208\n",
            "\n",
            "Epoch 00026: val_acc did not improve from 0.92490\n",
            "Epoch 27/30\n",
            "293/500 [================>.............] - ETA: 1:25 - loss: 0.0710 - acc: 0.9830"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 223s 445ms/step - loss: 0.0722 - acc: 0.9824 - val_loss: 0.3624 - val_acc: 0.9184\n",
            "\n",
            "Epoch 00027: val_acc did not improve from 0.92490\n",
            "Epoch 28/30\n",
            "308/500 [=================>............] - ETA: 1:19 - loss: 0.0720 - acc: 0.9826"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 222s 443ms/step - loss: 0.0714 - acc: 0.9826 - val_loss: 0.3543 - val_acc: 0.9206\n",
            "\n",
            "Epoch 00028: val_acc did not improve from 0.92490\n",
            "Epoch 29/30\n",
            "313/500 [=================>............] - ETA: 1:17 - loss: 0.0727 - acc: 0.9825"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 224s 449ms/step - loss: 0.0730 - acc: 0.9824 - val_loss: 0.3782 - val_acc: 0.9186\n",
            "\n",
            "Epoch 00029: val_acc did not improve from 0.92490\n",
            "Epoch 30/30\n",
            "314/500 [=================>............] - ETA: 1:17 - loss: 0.0729 - acc: 0.9820"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 221s 443ms/step - loss: 0.0708 - acc: 0.9825 - val_loss: 0.3351 - val_acc: 0.9240\n",
            "\n",
            "Epoch 00030: val_acc did not improve from 0.92490\n",
            "\n",
            "Epoch 00030: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-30e7a80a91bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mmodelB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"60to90.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"60to90.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0;34m'port'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m       \u001b[0;34m'path'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m       \u001b[0;34m'name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m   })\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result)\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     84\u001b[0m         reply.get('colab_msg_id') == message_id):\n\u001b[1;32m     85\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: TypeError: Failed to fetch"
          ]
        }
      ]
    }
  ]
}