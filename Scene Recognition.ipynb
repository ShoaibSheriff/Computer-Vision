{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.15"
    },
    "name": "HW3.ipynb",
    "colab": {
      "name": "CSE527_HW3_fall2018.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShoaibSheriff/Computer-Vision/blob/master/Scene%20Recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "autoscroll": false,
        "ein.tags": "worksheet-0",
        "id": "7CW9ClA4XnUE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import packages here\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import glob\n",
        "import itertools\n",
        "\n",
        "\n",
        "from sklearn.feature_extraction import image\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "autoscroll": false,
        "ein.tags": "worksheet-0",
        "scrolled": false,
        "id": "K3zLPL4PXnUN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class_names = [name[11:] for name in glob.glob('data/train/*')]\n",
        "class_names = dict(zip(xrange(len(class_names)), class_names))\n",
        "\n",
        "def load_dataset(path, num_per_class=-1):\n",
        "    data = []\n",
        "    labels = []\n",
        "    for id, class_name in class_names.iteritems():\n",
        "        img_path_class = glob.glob(path + class_name + '/*.jpg')\n",
        "        if num_per_class > 0:\n",
        "            img_path_class = img_path_class[:num_per_class]\n",
        "        labels.extend([id]*len(img_path_class))\n",
        "        for filename in img_path_class:\n",
        "            data.append(cv2.imread(filename, 0))\n",
        "    return data, labels\n",
        "\n",
        "# load training dataset\n",
        "train_data, train_label = load_dataset('data/train/')\n",
        "train_num = len(train_label)\n",
        "\n",
        "# load testing dataset\n",
        "test_data, test_label = load_dataset('data/test/', 100)\n",
        "test_num = len(test_label)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ein.tags": "worksheet-0",
        "id": "jyapFQaDXnUc",
        "colab_type": "text"
      },
      "source": [
        "## Tiny Image Representation + Nearest Neighbor Classifier\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "autoscroll": false,
        "ein.tags": "worksheet-0",
        "id": "ouATZTYFXnUd",
        "colab_type": "code",
        "colab": {},
        "outputId": "d117116a-9cea-46c7-c49c-99b4434aaf2f"
      },
      "source": [
        "%%time\n",
        "from sklearn import preprocessing\n",
        "tiny_img_dim = 16\n",
        "\n",
        "# load training dataset\n",
        "train_data, train_label = load_dataset('data/train/')\n",
        "train_num = len(train_label)\n",
        "\n",
        "# load testing dataset\n",
        "test_data, test_label = load_dataset('data/test/')\n",
        "test_num = len(test_label)\n",
        "\n",
        "\n",
        "# feature extraction\n",
        "def extract_feat(raw_data):\n",
        "    feat_dim = tiny_img_dim * tiny_img_dim\n",
        "    feat = np.zeros((len(raw_data), feat_dim), dtype=np.float32)\n",
        "    for i in xrange(feat.shape[0]):\n",
        "        tiny_img_i = cv2.resize(raw_data[i], (tiny_img_dim, tiny_img_dim))\n",
        "        temp = np.reshape(tiny_img_i, (1, tiny_img_dim * tiny_img_dim))\n",
        "        feat[i] = (temp - np.mean(temp)) / np.std(temp)\n",
        "    return feat\n",
        "\n",
        "train_feat = extract_feat(train_data)\n",
        "test_feat = extract_feat(test_data)\n",
        "\n",
        "# from sklearn import preprocessing\n",
        "# train_feat = preprocessing.normalize(train_feat)\n",
        "# test_feat = preprocessing.normalize(test_feat)\n",
        "\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "knn = KNeighborsClassifier(8)\n",
        "\n",
        "def train(X, Y):\n",
        "    knn.fit(X, Y)\n",
        "    return knn\n",
        "\n",
        "model = train(train_feat, train_label)\n",
        "\n",
        "# prediction: take feature and model, return label\n",
        "def predict(model, x):\n",
        "    return model.predict(x) # dummy implementation\n",
        "\n",
        "predictions = predict(model, test_feat)\n",
        "\n",
        "accuracy = sum(np.array(predictions) == test_label) / float(test_num)\n",
        "\n",
        "pred1, label1 = predictions, test_label\n",
        "print \"The accuracy of my dummy model is {:.2f}%\".format(accuracy*100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The accuracy of my dummy model is 22.24%\n",
            "CPU times: user 5.89 s, sys: 393 ms, total: 6.28 s\n",
            "Wall time: 11.9 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkUO3ZyXXnUj",
        "colab_type": "text"
      },
      "source": [
        "## Metrics\n",
        "Accuracy : 22.24%\n",
        "Time : 5.44 s"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ein.tags": "worksheet-0",
        "id": "3GudIz1uXnUs",
        "colab_type": "text"
      },
      "source": [
        "## Bag of SIFT Representation + Nearest Neighbor Classifer\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "autoscroll": false,
        "ein.tags": "worksheet-0",
        "id": "EZw98hBxXnUt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "readFromPickle = True\n",
        "\n",
        "import pickle\n",
        "sift = cv2.xfeatures2d.SIFT_create()\n",
        "\n",
        "from sklearn.feature_extraction.image import extract_patches_2d\n",
        "\n",
        "# load training dataset\n",
        "train_data, train_label = load_dataset('data/train/')\n",
        "train_num = len(train_label)\n",
        "\n",
        "# load testing dataset\n",
        "test_data, test_label = load_dataset('data/test/')\n",
        "test_num = len(test_label)\n",
        "\n",
        "\n",
        "def buildDictonary(X_train) :\n",
        "    \n",
        "    #all sift features across training images\n",
        "    total_sift_features = np.zeros(shape=(0,128))\n",
        "    step_size = 24\n",
        "\n",
        "    for img_idx in range(len(X_train)) :\n",
        "#     for img_idx in range(300) :\n",
        "        img = X_train[img_idx]\n",
        "\n",
        "#         patches = extract_patches_2d(img, (step_size, step_size), max_patches = 1000)\n",
        "#         kp = [cv2.KeyPoint(x, y, step_size) for y in range(0, patches.shape[0], step_size) \n",
        "#                         for x in range(0, patches.shape[1], step_size)]\n",
        "    \n",
        "        kp = [cv2.KeyPoint(x, y, step_size) for x in range(0, img.shape[0], step_size) \n",
        "                        for y in range(0, img.shape[1], step_size)]\n",
        "        \n",
        "        #676 kps per image. Each descriptor is 128\n",
        "        keypoints, dense_feat = sift.compute(img, kp)\n",
        "        total_sift_features = np.vstack([total_sift_features, dense_feat])\n",
        "        \n",
        "    from sklearn.cluster import KMeans\n",
        "    kmeans = KMeans(n_clusters = 80, random_state = 0,  n_jobs = 3)\n",
        "    kmeans.fit(total_sift_features)\n",
        "    return kmeans;\n",
        "\n",
        "\n",
        "def loadDict(f_name) :\n",
        "\n",
        "    f = open(f_name, 'rb')\n",
        "    obj = pickle.load(f)\n",
        "    f.close()\n",
        "    return obj\n",
        "\n",
        "def saveDict(model, f_name) :\n",
        "\n",
        "    f = open(f_name, 'wb')\n",
        "    pickle.dump(model, f)\n",
        "    f.close()\n",
        "    return\n",
        "\n",
        "# Disable this is readin from model.\n",
        "if (readFromPickle == False) :\n",
        "    model = buildDictonary(train_data)\n",
        "    saveDict(model, 'dict_step_24_cl_80.pkl')\n",
        "\n",
        "if (readFromPickle == True) :\n",
        "    model = loadDict('dict_step_24_cl_80.pkl')\n",
        "    \n",
        "\n",
        "def getBagOfWordsRepresentation(dictionary, X) :\n",
        "    \n",
        "    total_sift_features_per_image = []\n",
        "    X_bow_per_image = np.zeros((0, 80))\n",
        "    \n",
        "    step_size = 4\n",
        "\n",
        "    for img_idx in range(len(X)) :\n",
        "#     for img_idx in range(300) :\n",
        "\n",
        "        img = X[img_idx]\n",
        "    \n",
        "    \n",
        "#         patches = extract_patches_2d(img, (step_size, step_size), max_patches = 1000)\n",
        "\n",
        "#         kp = [cv2.KeyPoint(x, y, step_size) for y in range(0, patches.shape[0], step_size) \n",
        "#                         for x in range(0, patches.shape[1], step_size)]\n",
        "        \n",
        "        #676 kps per image. Each descriptor is 128\n",
        "        kp = [cv2.KeyPoint(x, y, step_size) for x in range(0, img.shape[0], step_size) \n",
        "                        for y in range(0, img.shape[1], step_size)]\n",
        "        \n",
        "        #676 kps per image. Each descriptor is 128\n",
        "        keypoints, dense_feat = sift.compute(img, kp)\n",
        "        X_bows =  dictionary.predict(dense_feat)\n",
        "        hist, bins = np.histogram(X_bows, bins=80)\n",
        "        hist = hist.astype('float32')\n",
        "        hist = (hist - hist.mean()) / hist.std()\n",
        "#         hist = hist/np.sum(hist)\n",
        "        \n",
        "        X_bow_per_image = np.vstack([X_bow_per_image, hist])\n",
        "\n",
        "    return X_bow_per_image\n",
        "   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Khvdlm20XnUz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load training dataset\n",
        "train_data, train_label = load_dataset('data/train/', 70)\n",
        "train_num = len(train_label)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urJlWl8gXnU3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "X_train_bow = getBagOfWordsRepresentation(model, train_data)\n",
        "print(X_train_bow.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eqKKg0rNXnU6",
        "colab_type": "code",
        "colab": {},
        "outputId": "866d71d1-8a53-4ebc-b807-8944b5240385"
      },
      "source": [
        "%%time\n",
        "X_test_bow = getBagOfWordsRepresentation(model, test_data)\n",
        "print(X_test_bow.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2985, 80)\n",
            "CPU times: user 16min 5s, sys: 7min 53s, total: 23min 59s\n",
            "Wall time: 6min 11s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnxaKahmXnU_",
        "colab_type": "code",
        "colab": {},
        "outputId": "0c3f348f-2a20-4751-d580-12231271085e"
      },
      "source": [
        "%%time\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "knn = KNeighborsClassifier(4)\n",
        "def train(model, X, Y):\n",
        "    knn.fit(X, Y)\n",
        "    return knn\n",
        "\n",
        "model = train(knn, X_train_bow, train_label)\n",
        "\n",
        "# prediction: take feature and model, return label\n",
        "def predict(model, x):\n",
        "    return model.predict(x) # dummy implementation\n",
        "\n",
        "predictions = predict(model, X_test_bow)\n",
        "\n",
        "accuracy = sum(np.array(predictions) == test_label) / float(test_num)\n",
        "\n",
        "pred2, label2 = predictions, test_label\n",
        "print \"The accuracy of my dummy model is {:.2f}%\".format(accuracy * 100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The accuracy of my dummy model is 51.42%\n",
            "CPU times: user 336 ms, sys: 0 ns, total: 336 ms\n",
            "Wall time: 336 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CapOYgLpXnVD",
        "colab_type": "text"
      },
      "source": [
        "## Metrics\n",
        "Accuracy : 51.42% <br>\n",
        "Best value for k = 80(k means), step size = 24(dense keypoints), k(knn) = 4. For training phase, step size of 4 is used with 70% data(for memory issues).<br>\n",
        "Time to build dictionary : 35min 17s <br>\n",
        "Time to get train data bag of words representation : 7min 59s <br>\n",
        "Time to get test data bag of words representation : 23min 59s <br>\n",
        "Time to predict with knn :  336 ms <br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ein.tags": "worksheet-0",
        "id": "ga7pAFd-XnVL",
        "colab_type": "text"
      },
      "source": [
        "## Bag of SIFT Representation + one-vs-all SVMs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "autoscroll": false,
        "ein.tags": "worksheet-0",
        "id": "TjjSjtZvXnVN",
        "colab_type": "code",
        "colab": {},
        "outputId": "3e1fab2d-d4dd-4a14-e734-15da0f2ac949"
      },
      "source": [
        "readFromPickle = True\n",
        "\n",
        "import pickle\n",
        "sift = cv2.xfeatures2d.SIFT_create()\n",
        "\n",
        "from sklearn.feature_extraction.image import extract_patches_2d\n",
        "\n",
        "# load training dataset\n",
        "train_data, train_label = load_dataset('data/train/')\n",
        "train_num = len(train_label)\n",
        "\n",
        "# load testing dataset\n",
        "test_data, test_label = load_dataset('data/test/')\n",
        "test_num = len(test_label)\n",
        "\n",
        "\n",
        "def buildDictonary(X_train) :\n",
        "    \n",
        "    #all sift features across training images\n",
        "    total_sift_features = np.zeros(shape=(0,128))\n",
        "    step_size = 24\n",
        "\n",
        "    for img_idx in range(len(X_train)) :\n",
        "#     for img_idx in range(300) :\n",
        "        img = X_train[img_idx]\n",
        "\n",
        "#         patches = extract_patches_2d(img, (step_size, step_size), max_patches = 1000)\n",
        "#         kp = [cv2.KeyPoint(x, y, step_size) for y in range(0, patches.shape[0], step_size) \n",
        "#                         for x in range(0, patches.shape[1], step_size)]\n",
        "    \n",
        "        kp = [cv2.KeyPoint(x, y, step_size) for x in range(0, img.shape[0], step_size) \n",
        "                        for y in range(0, img.shape[1], step_size)]\n",
        "        \n",
        "        #676 kps per image. Each descriptor is 128\n",
        "        keypoints, dense_feat = sift.compute(img, kp)\n",
        "        total_sift_features = np.vstack([total_sift_features, dense_feat])\n",
        "        \n",
        "    from sklearn.cluster import KMeans\n",
        "    kmeans = KMeans(n_clusters = 80, random_state = 0,  n_jobs = 3)\n",
        "    kmeans.fit(total_sift_features)\n",
        "    return kmeans;\n",
        "\n",
        "\n",
        "def loadDict(f_name) :\n",
        "\n",
        "    f = open(f_name, 'rb')\n",
        "    obj = pickle.load(f)\n",
        "    f.close()\n",
        "    return obj\n",
        "\n",
        "def saveDict(model, f_name) :\n",
        "\n",
        "    f = open(f_name, 'wb')\n",
        "    pickle.dump(model, f)\n",
        "    f.close()\n",
        "    return\n",
        "\n",
        "# Disable this is readin from model.\n",
        "if (readFromPickle == False) :\n",
        "    model = buildDictonary(train_data)\n",
        "    saveDict(model, 'dict_step_24_cl_80.pkl')\n",
        "\n",
        "if (readFromPickle == True) :\n",
        "    model = loadDict('dict_step_24_cl_80.pkl')\n",
        "    \n",
        "\n",
        "def getBagOfWordsRepresentation(dictionary, X) :\n",
        "    \n",
        "    total_sift_features_per_image = []\n",
        "    X_bow_per_image = np.zeros((0, 80))\n",
        "    \n",
        "    step_size = 4\n",
        "\n",
        "    for img_idx in range(len(X)) :\n",
        "#     for img_idx in range(300) :\n",
        "\n",
        "        img = X[img_idx]\n",
        "    \n",
        "    \n",
        "#         patches = extract_patches_2d(img, (step_size, step_size), max_patches = 1000)\n",
        "\n",
        "#         kp = [cv2.KeyPoint(x, y, step_size) for y in range(0, patches.shape[0], step_size) \n",
        "#                         for x in range(0, patches.shape[1], step_size)]\n",
        "        \n",
        "        #676 kps per image. Each descriptor is 128\n",
        "        kp = [cv2.KeyPoint(x, y, step_size) for x in range(0, img.shape[0], step_size) \n",
        "                        for y in range(0, img.shape[1], step_size)]\n",
        "        \n",
        "        #676 kps per image. Each descriptor is 128\n",
        "        keypoints, dense_feat = sift.compute(img, kp)\n",
        "        X_bows =  dictionary.predict(dense_feat)\n",
        "        hist, bins = np.histogram(X_bows, bins=80)\n",
        "        hist = hist.astype('float32')\n",
        "        hist = (hist - hist.mean()) / hist.std()\n",
        "#         hist = hist/np.sum(hist)\n",
        "        \n",
        "        X_bow_per_image = np.vstack([X_bow_per_image, hist])\n",
        "\n",
        "    return X_bow_per_image"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ends here\n",
            "CPU times: user 2.7 s, sys: 151 ms, total: 2.85 s\n",
            "Wall time: 2.85 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-kHY8kHXnVQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load training dataset\n",
        "train_data, train_label = load_dataset('data/train/', 70)\n",
        "train_num = len(train_label)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3kyLajSXnVU",
        "colab_type": "code",
        "colab": {},
        "outputId": "19b0a662-101e-4636-fd2f-18ea7c22541c"
      },
      "source": [
        "%%time\n",
        "X_train_bow = getBagOfWordsRepresentation(model, train_data)\n",
        "print(X_train_bow.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1050, 80)\n",
            "CPU times: user 5min 42s, sys: 2min 47s, total: 8min 29s\n",
            "Wall time: 2min 11s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xVYfQTcXnVX",
        "colab_type": "code",
        "colab": {},
        "outputId": "aace5994-f7ad-409c-b3d5-8c44ebef5de4"
      },
      "source": [
        "%%time\n",
        "X_test_bow = getBagOfWordsRepresentation(model, test_data)\n",
        "print(X_test_bow.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2985, 80)\n",
            "CPU times: user 16min 50s, sys: 7min 48s, total: 24min 38s\n",
            "Wall time: 6min 23s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "duKw779MXnVb",
        "colab_type": "code",
        "colab": {},
        "outputId": "96bcd89a-0fc5-4848-c6c2-6e1bafe89b40"
      },
      "source": [
        "%%time\n",
        "# train_labels_split = []\n",
        "# for i in range(len(train_label)):\n",
        "#     train_labels_split.append(encodeLabelsForClass(train_label, i))\n",
        "\n",
        "# from sklearn.metrics.pairwise import chi2_kernel\n",
        "# K_tr = chi2_kernel([X_train_bow, X_train_bow], gamma=10)\n",
        "# K_tst = chi2_kernel(X_test_bow, gamma=10)\n",
        "\n",
        "\n",
        "# training a linear SVM classifier \n",
        "from sklearn.svm import  SVC \n",
        "svm_model_linear = SVC(kernel='linear', C = 1.19)\n",
        "svm_model_linear.fit(X_train_bow, train_label) \n",
        "svm_predictions = svm_model_linear.predict(X_test_bow) \n",
        "\n",
        "accuracy = sum(np.array(svm_predictions) == test_label) / float(test_num)\n",
        "print \"The accuracy of  model is {:.2f}%\".format(accuracy*100)\n",
        "\n",
        "pred3, label3 = svm_predictions, test_label"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The accuracy of  model is 61.07%\n",
            "CPU times: user 382 ms, sys: 59.1 ms, total: 441 ms\n",
            "Wall time: 382 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4fIXNx0XnVf",
        "colab_type": "text"
      },
      "source": [
        "## Metrics\n",
        "Accuracy : 61.07% <br>\n",
        "Found best value for C = 1.19, step size = 24 and k = 80. For training phase, step size of 4 is used with 70% data(for memory issues)..(due to memory issues)<br>\n",
        "Time to build dictionary : 35min 17s <br>\n",
        "Time to get train data bag of words representation : 10min 59s<br>\n",
        "Time to get test data bag of words representation : 20min 31s <br>\n",
        "Time to predict with knn :  335 ms <br>"
      ]
    }
  ]
}